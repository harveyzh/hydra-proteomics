#
# properties file for Hydra Launcher
#
#
# may be specified on the command line as well
#
# If using a cluster this is the hdfs directory    if unspecified
# will run locally
# use line below to run remotely
 #use line below to run on the cluster
#remoteBaseDirectory=/user/howdah/JXTandem/data/<LOCAL_DIRECTORY>
#use line below to run on the ec2
#remoteBaseDirectory=s3n://lordjoe/<LOCAL_DIRECTORY>
#
#  hdfs job tracker
remoteHost=glados
#
#  port on the job tracker
remotePort= 9000
#
#  Actual job tracker with port - not the same as name node port
remoteJobTracker= glados:9001
#
#  user on the job tracker
remoteUser=slewis
#
#  encrypted password  on the job tracker alternative is plainTextRemotePassword
#encryptedRemotePassword=aWnQ4DbdoSGivLcfi56hGKK8tx+LnqEYory3H4ueoRiivLcfi56hGA==
#
#  plain password  on the job tracker alternative is encryptedRemotePassword
#plainTextRemotePassword=secret

#
#
# Maximum number of peptides to handle in a reducer - raising this will raise the memory requirements on the cluster
# default is 20000
maxPeptideFragmentsPerReducer=30000

#
# sets mapres.max.splt.size
#  a lower number forces more mappers in my splitters which is good
# the default is 64mb but for what we are doing smaller seems to be better
# when mappers do a lot of work - this is 16 megs
#maxSplitSize=16777216
#maxSplitSize=20000000

#
# MaxReduceTasks tunes the cluster - increase this number for
# bigger clusters this number usually works well at approximately the number of cores
maxReduceTasks = 36

#
#
# delete hadoop directories after the process is finished with them
# set fo false if debugging and the intermediate data wants to be examined
deleteOutputDirectories=true
 #
 # compressed files take less space and are harder to read
# set false if you plan to manually read intermediate files
compressIntermediateFiles=true
 #
 # Maximum memory assigned to child tasks  in megabytes
 # maps to "mapred.child.java.opts", "-Xmx" + maxMamory + "m"
maxClusterMemory=3600

#
# override specific Hadoop prepoerties - save writing specific code
DEFINE_io.sort.factor=100
DEFINE_io.sort.mb=400
# don't start reducers until most mappers done
DEFINE_mapred.reduce.slowstart.completed.maps=0.5
#  number of map tasks for the database
DEFINE_org.systemsbiology.jxtandem.DesiredDatabaseMappers=8
#  number of map tasks for the spectra
DEFINE_org.systemsbiology.jxtandem.DesiredXMLInputMappers=7
#  maximum number of modifications per paptide
DEFINE_org.systemsbiology.jxtandem.ModifiedPolypeptide.MaxPeptideModifications=2
# use hard coded modification like cystein
DEFINE_org.systemsbiology.xtandem.HardCodeModifications=no
# save a scans file
DEFINE_org.systemsbiology.xtandem.SaveScansData=yes
# Write out PepXML
DEFINE_org.systemsbiology.xtandem.hadoop.WritePepXML=yes
# Write out MFGData if the hyperscore is greater than this value
DEFINE_org.systemsbiology.xtandem.hadoop.WriteMGFSpectraWithHyperscoreGreaterThan=200
# Write out MFGData if the hyperscore is greater than this value
DEFINE_org.systemsbiology.xtandem.hadoop.WriteMGFSpectraWithExpectValueLowerThan=2
# algorithms
#DEFINE_org.systemsbiology.algorithm=org.systemsbiology.xtandem.morpheus.MorpheusScoringAlgorithm;org.systemsbiology.xtandem.TandemKScoringAlgorithm;org.systemsbiology.xtandem.probidx.ProbIdScoringAlgorithm
#DEFINE_org.systemsbiology.algorithm=org.systemsbiology.xtandem.TandemKScoringAlgorithm
DEFINE_org.systemsbiology.algorithm=org.systemsbiology.xtandem.comet.CometScoringAlgorithm

#number matches that are remembered
DEFINE_org.systemsbiology.numberRememberedMatches=4

#DEFINE_org.systemsbiology.algorithm=org.systemsbiology.xtandem.comet.CometScoringAlgorithm
# create decoy peptides by reversing the amino acids
#DEFINE_org.systemsbiology.xtandem.CreateDecoyPeptides=yes
# when yes each input file goes to a separate output file
DEFINE_org.systemsbiology.xtandem.MultipleOutputFiles=yes
# fix a nasty bug in 1.03
DEFINE_java.net.preferIPv4Stack=true
# binning to use on fragment ions
DEFINE_comet.fragment_bin_tol=0.03
# offset position to start the binning
DEFINE_comet.fragment_bin_offset=0.00
# tolerance to mass difference  in this case 10 ppm
DEFINE_comet.mass_tolerance=0.01
#
# Used only when using Amazon Elastic Map Reduce
# try spot instances
# try spot instances
DEFINE_org.systemsbiology.aws.UseSpotMaster=yes
# use 6 instances
DEFINE_org.systemsbiology.aws.ClusterSize=9
# keep the cluster running
DEFINE_org.systemsbiology.aws.KeepClusterAfterJob=yes
# use this jar
#DEFINE_org.systemsbiology.tandem.hadoop.PrebuiltJar=Mar300846_0.jar
DEFINE_org.systemsbiology.aws.InstanceSize=large
#if true then leave files on server and do not copy
#DEFINE_org.systemsbiology.xtandem.DoNotCopyFilesToLocalMachine=no
